{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrayfire as af\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toArrayFire(x):\n",
    "    x_np = x.detach().contiguous().numpy()\n",
    "    shape = 1\n",
    "    if len(x_np.shape) == 0:\n",
    "        shape = (1,)\n",
    "    else:\n",
    "        shape = x_np.shape[::-1]\n",
    "    afArray = af.Array(x_np.ctypes.data, shape, x_np.dtype.char)\n",
    "    return afArray\n",
    "\n",
    "def saveStateDict(model, filepath):\n",
    "    params = {}\n",
    "    i = 0\n",
    "    for (name, param) in model.state_dict().items():\n",
    "        if 'running' in name:\n",
    "            continue\n",
    "        if 'in_proj' in name:\n",
    "            q, k, v = param.chunk(3, dim=0)\n",
    "            hack = '0'\n",
    "            if 'in_proj_bias' in name: hack = '1'\n",
    "            params['0q_' + hack + name] = q\n",
    "            params['1k_' + hack + name] = k\n",
    "            params['2v_' + hack + name] = v\n",
    "            if 'in_proj_bias' in name:\n",
    "                for key in sorted(params.keys()):\n",
    "                    af_array = toArrayFire(params[key])\n",
    "                    if 'weight' in key:\n",
    "                        af_array = af.array.transpose(af_array)\n",
    "                    print(key, i, params[key].shape)\n",
    "                    print(af.array.save_array(key, af_array, filepath, True))\n",
    "                    i = i + 1\n",
    "                params = {}\n",
    "            continue\n",
    "        elif len(param.size()) > 0:\n",
    "            if 'input_proj.bias' in name:\n",
    "                param = param.reshape((1, 1, 256))\n",
    "            af_array = toArrayFire(param)\n",
    "            if 'fc' in name and 'weight' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'proj' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'weight' in name and 'linear' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "            elif 'query_embed' in name:\n",
    "                af_array = af_array\n",
    "            elif 'weight' in name and 'embed' in name:\n",
    "                af_array = af.array.transpose(af_array)\n",
    "\n",
    "            print(name, i, param.shape)\n",
    "            print(af.array.save_array(name, af_array, filepath, True))\n",
    "            i = i + 1\n",
    "    for name in model.state_dict():\n",
    "        if 'running' in name:\n",
    "            print(name)\n",
    "            af_array = toArrayFire(model.state_dict()[name])\n",
    "            print(name, model.state_dict()[name].shape, af.array.save_array(name, af_array, filepath + 'running', True))\n",
    "    \n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--optimizer', default=\"adam\", type=str)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "    parser.add_argument('--eval_skip', default=1, type=int,\n",
    "                        help='do evaluation every \"eval_skip\" frames')\n",
    "    parser.add_argument('--schedule', default='step', type=str,\n",
    "                        choices=('step', 'multistep'))\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=100, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--no_pass_pos_and_query', dest='pass_pos_and_query', action='store_false',\n",
    "                        help=\"Disables passing the positional encodings to each attention layers\")\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--mask_model', default='none', type=str, choices=(\"none\", \"smallconv\", \"v2\"),\n",
    "                        help=\"Segmentation head to be used (if None, segmentation will not be trained)\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--set_loss', default='hungarian', type=str,\n",
    "                        choices=('sequential', 'hungarian', 'lexicographical'),\n",
    "                        help=\"Type of matching to perform in the loss\")\n",
    "    parser.add_argument('--bcl', dest='use_bcl', action='store_true',\n",
    "                        help=\"Use balanced classification loss\")\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/datasets01/COCO/022719')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str, default='/datasets01/COCO/060419')\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--masks', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output-dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.detr import *\n",
    "parser = create_parser()\n",
    "pretrained_path = '/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth'\n",
    "args = parser.parse_args([\"--resume=/private/home/padentomasello/scratch/pytorch_testing/detr-r50-e632da11.pth\"])\n",
    "model, _, _ = build(args)   \n",
    "if args.resume:\n",
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.body.conv1.weight 0 torch.Size([64, 3, 7, 7])\n",
      "2\n",
      "0.body.bn1.weight 1 torch.Size([64])\n",
      "3\n",
      "0.body.bn1.bias 2 torch.Size([64])\n",
      "4\n",
      "0.body.layer1.0.conv1.weight 3 torch.Size([64, 64, 1, 1])\n",
      "5\n",
      "0.body.layer1.0.bn1.weight 4 torch.Size([64])\n",
      "6\n",
      "0.body.layer1.0.bn1.bias 5 torch.Size([64])\n",
      "7\n",
      "0.body.layer1.0.conv2.weight 6 torch.Size([64, 64, 3, 3])\n",
      "8\n",
      "0.body.layer1.0.bn2.weight 7 torch.Size([64])\n",
      "9\n",
      "0.body.layer1.0.bn2.bias 8 torch.Size([64])\n",
      "10\n",
      "0.body.layer1.0.conv3.weight 9 torch.Size([256, 64, 1, 1])\n",
      "11\n",
      "0.body.layer1.0.bn3.weight 10 torch.Size([256])\n",
      "12\n",
      "0.body.layer1.0.bn3.bias 11 torch.Size([256])\n",
      "13\n",
      "0.body.layer1.0.downsample.0.weight 12 torch.Size([256, 64, 1, 1])\n",
      "14\n",
      "0.body.layer1.0.downsample.1.weight 13 torch.Size([256])\n",
      "15\n",
      "0.body.layer1.0.downsample.1.bias 14 torch.Size([256])\n",
      "16\n",
      "0.body.layer1.1.conv1.weight 15 torch.Size([64, 256, 1, 1])\n",
      "17\n",
      "0.body.layer1.1.bn1.weight 16 torch.Size([64])\n",
      "18\n",
      "0.body.layer1.1.bn1.bias 17 torch.Size([64])\n",
      "19\n",
      "0.body.layer1.1.conv2.weight 18 torch.Size([64, 64, 3, 3])\n",
      "20\n",
      "0.body.layer1.1.bn2.weight 19 torch.Size([64])\n",
      "21\n",
      "0.body.layer1.1.bn2.bias 20 torch.Size([64])\n",
      "22\n",
      "0.body.layer1.1.conv3.weight 21 torch.Size([256, 64, 1, 1])\n",
      "23\n",
      "0.body.layer1.1.bn3.weight 22 torch.Size([256])\n",
      "24\n",
      "0.body.layer1.1.bn3.bias 23 torch.Size([256])\n",
      "25\n",
      "0.body.layer1.2.conv1.weight 24 torch.Size([64, 256, 1, 1])\n",
      "26\n",
      "0.body.layer1.2.bn1.weight 25 torch.Size([64])\n",
      "27\n",
      "0.body.layer1.2.bn1.bias 26 torch.Size([64])\n",
      "28\n",
      "0.body.layer1.2.conv2.weight 27 torch.Size([64, 64, 3, 3])\n",
      "29\n",
      "0.body.layer1.2.bn2.weight 28 torch.Size([64])\n",
      "30\n",
      "0.body.layer1.2.bn2.bias 29 torch.Size([64])\n",
      "31\n",
      "0.body.layer1.2.conv3.weight 30 torch.Size([256, 64, 1, 1])\n",
      "32\n",
      "0.body.layer1.2.bn3.weight 31 torch.Size([256])\n",
      "33\n",
      "0.body.layer1.2.bn3.bias 32 torch.Size([256])\n",
      "34\n",
      "0.body.layer2.0.conv1.weight 33 torch.Size([128, 256, 1, 1])\n",
      "35\n",
      "0.body.layer2.0.bn1.weight 34 torch.Size([128])\n",
      "36\n",
      "0.body.layer2.0.bn1.bias 35 torch.Size([128])\n",
      "37\n",
      "0.body.layer2.0.conv2.weight 36 torch.Size([128, 128, 3, 3])\n",
      "38\n",
      "0.body.layer2.0.bn2.weight 37 torch.Size([128])\n",
      "39\n",
      "0.body.layer2.0.bn2.bias 38 torch.Size([128])\n",
      "40\n",
      "0.body.layer2.0.conv3.weight 39 torch.Size([512, 128, 1, 1])\n",
      "41\n",
      "0.body.layer2.0.bn3.weight 40 torch.Size([512])\n",
      "42\n",
      "0.body.layer2.0.bn3.bias 41 torch.Size([512])\n",
      "43\n",
      "0.body.layer2.0.downsample.0.weight 42 torch.Size([512, 256, 1, 1])\n",
      "44\n",
      "0.body.layer2.0.downsample.1.weight 43 torch.Size([512])\n",
      "45\n",
      "0.body.layer2.0.downsample.1.bias 44 torch.Size([512])\n",
      "46\n",
      "0.body.layer2.1.conv1.weight 45 torch.Size([128, 512, 1, 1])\n",
      "47\n",
      "0.body.layer2.1.bn1.weight 46 torch.Size([128])\n",
      "48\n",
      "0.body.layer2.1.bn1.bias 47 torch.Size([128])\n",
      "49\n",
      "0.body.layer2.1.conv2.weight 48 torch.Size([128, 128, 3, 3])\n",
      "50\n",
      "0.body.layer2.1.bn2.weight 49 torch.Size([128])\n",
      "51\n",
      "0.body.layer2.1.bn2.bias 50 torch.Size([128])\n",
      "52\n",
      "0.body.layer2.1.conv3.weight 51 torch.Size([512, 128, 1, 1])\n",
      "53\n",
      "0.body.layer2.1.bn3.weight 52 torch.Size([512])\n",
      "54\n",
      "0.body.layer2.1.bn3.bias 53 torch.Size([512])\n",
      "55\n",
      "0.body.layer2.2.conv1.weight 54 torch.Size([128, 512, 1, 1])\n",
      "56\n",
      "0.body.layer2.2.bn1.weight 55 torch.Size([128])\n",
      "57\n",
      "0.body.layer2.2.bn1.bias 56 torch.Size([128])\n",
      "58\n",
      "0.body.layer2.2.conv2.weight 57 torch.Size([128, 128, 3, 3])\n",
      "59\n",
      "0.body.layer2.2.bn2.weight 58 torch.Size([128])\n",
      "60\n",
      "0.body.layer2.2.bn2.bias 59 torch.Size([128])\n",
      "61\n",
      "0.body.layer2.2.conv3.weight 60 torch.Size([512, 128, 1, 1])\n",
      "62\n",
      "0.body.layer2.2.bn3.weight 61 torch.Size([512])\n",
      "63\n",
      "0.body.layer2.2.bn3.bias 62 torch.Size([512])\n",
      "64\n",
      "0.body.layer2.3.conv1.weight 63 torch.Size([128, 512, 1, 1])\n",
      "65\n",
      "0.body.layer2.3.bn1.weight 64 torch.Size([128])\n",
      "66\n",
      "0.body.layer2.3.bn1.bias 65 torch.Size([128])\n",
      "67\n",
      "0.body.layer2.3.conv2.weight 66 torch.Size([128, 128, 3, 3])\n",
      "68\n",
      "0.body.layer2.3.bn2.weight 67 torch.Size([128])\n",
      "69\n",
      "0.body.layer2.3.bn2.bias 68 torch.Size([128])\n",
      "70\n",
      "0.body.layer2.3.conv3.weight 69 torch.Size([512, 128, 1, 1])\n",
      "71\n",
      "0.body.layer2.3.bn3.weight 70 torch.Size([512])\n",
      "72\n",
      "0.body.layer2.3.bn3.bias 71 torch.Size([512])\n",
      "73\n",
      "0.body.layer3.0.conv1.weight 72 torch.Size([256, 512, 1, 1])\n",
      "74\n",
      "0.body.layer3.0.bn1.weight 73 torch.Size([256])\n",
      "75\n",
      "0.body.layer3.0.bn1.bias 74 torch.Size([256])\n",
      "76\n",
      "0.body.layer3.0.conv2.weight 75 torch.Size([256, 256, 3, 3])\n",
      "77\n",
      "0.body.layer3.0.bn2.weight 76 torch.Size([256])\n",
      "78\n",
      "0.body.layer3.0.bn2.bias 77 torch.Size([256])\n",
      "79\n",
      "0.body.layer3.0.conv3.weight 78 torch.Size([1024, 256, 1, 1])\n",
      "80\n",
      "0.body.layer3.0.bn3.weight 79 torch.Size([1024])\n",
      "81\n",
      "0.body.layer3.0.bn3.bias 80 torch.Size([1024])\n",
      "82\n",
      "0.body.layer3.0.downsample.0.weight 81 torch.Size([1024, 512, 1, 1])\n",
      "83\n",
      "0.body.layer3.0.downsample.1.weight 82 torch.Size([1024])\n",
      "84\n",
      "0.body.layer3.0.downsample.1.bias 83 torch.Size([1024])\n",
      "85\n",
      "0.body.layer3.1.conv1.weight 84 torch.Size([256, 1024, 1, 1])\n",
      "86\n",
      "0.body.layer3.1.bn1.weight 85 torch.Size([256])\n",
      "87\n",
      "0.body.layer3.1.bn1.bias 86 torch.Size([256])\n",
      "88\n",
      "0.body.layer3.1.conv2.weight 87 torch.Size([256, 256, 3, 3])\n",
      "89\n",
      "0.body.layer3.1.bn2.weight 88 torch.Size([256])\n",
      "90\n",
      "0.body.layer3.1.bn2.bias 89 torch.Size([256])\n",
      "91\n",
      "0.body.layer3.1.conv3.weight 90 torch.Size([1024, 256, 1, 1])\n",
      "92\n",
      "0.body.layer3.1.bn3.weight 91 torch.Size([1024])\n",
      "93\n",
      "0.body.layer3.1.bn3.bias 92 torch.Size([1024])\n",
      "94\n",
      "0.body.layer3.2.conv1.weight 93 torch.Size([256, 1024, 1, 1])\n",
      "95\n",
      "0.body.layer3.2.bn1.weight 94 torch.Size([256])\n",
      "96\n",
      "0.body.layer3.2.bn1.bias 95 torch.Size([256])\n",
      "97\n",
      "0.body.layer3.2.conv2.weight 96 torch.Size([256, 256, 3, 3])\n",
      "98\n",
      "0.body.layer3.2.bn2.weight 97 torch.Size([256])\n",
      "99\n",
      "0.body.layer3.2.bn2.bias 98 torch.Size([256])\n",
      "100\n",
      "0.body.layer3.2.conv3.weight 99 torch.Size([1024, 256, 1, 1])\n",
      "101\n",
      "0.body.layer3.2.bn3.weight 100 torch.Size([1024])\n",
      "102\n",
      "0.body.layer3.2.bn3.bias 101 torch.Size([1024])\n",
      "103\n",
      "0.body.layer3.3.conv1.weight 102 torch.Size([256, 1024, 1, 1])\n",
      "104\n",
      "0.body.layer3.3.bn1.weight 103 torch.Size([256])\n",
      "105\n",
      "0.body.layer3.3.bn1.bias 104 torch.Size([256])\n",
      "106\n",
      "0.body.layer3.3.conv2.weight 105 torch.Size([256, 256, 3, 3])\n",
      "107\n",
      "0.body.layer3.3.bn2.weight 106 torch.Size([256])\n",
      "108\n",
      "0.body.layer3.3.bn2.bias 107 torch.Size([256])\n",
      "109\n",
      "0.body.layer3.3.conv3.weight 108 torch.Size([1024, 256, 1, 1])\n",
      "110\n",
      "0.body.layer3.3.bn3.weight 109 torch.Size([1024])\n",
      "111\n",
      "0.body.layer3.3.bn3.bias 110 torch.Size([1024])\n",
      "112\n",
      "0.body.layer3.4.conv1.weight 111 torch.Size([256, 1024, 1, 1])\n",
      "113\n",
      "0.body.layer3.4.bn1.weight 112 torch.Size([256])\n",
      "114\n",
      "0.body.layer3.4.bn1.bias 113 torch.Size([256])\n",
      "115\n",
      "0.body.layer3.4.conv2.weight 114 torch.Size([256, 256, 3, 3])\n",
      "116\n",
      "0.body.layer3.4.bn2.weight 115 torch.Size([256])\n",
      "117\n",
      "0.body.layer3.4.bn2.bias 116 torch.Size([256])\n",
      "118\n",
      "0.body.layer3.4.conv3.weight 117 torch.Size([1024, 256, 1, 1])\n",
      "119\n",
      "0.body.layer3.4.bn3.weight 118 torch.Size([1024])\n",
      "120\n",
      "0.body.layer3.4.bn3.bias 119 torch.Size([1024])\n",
      "121\n",
      "0.body.layer3.5.conv1.weight 120 torch.Size([256, 1024, 1, 1])\n",
      "122\n",
      "0.body.layer3.5.bn1.weight 121 torch.Size([256])\n",
      "123\n",
      "0.body.layer3.5.bn1.bias 122 torch.Size([256])\n",
      "124\n",
      "0.body.layer3.5.conv2.weight 123 torch.Size([256, 256, 3, 3])\n",
      "125\n",
      "0.body.layer3.5.bn2.weight 124 torch.Size([256])\n",
      "126\n",
      "0.body.layer3.5.bn2.bias 125 torch.Size([256])\n",
      "127\n",
      "0.body.layer3.5.conv3.weight 126 torch.Size([1024, 256, 1, 1])\n",
      "128\n",
      "0.body.layer3.5.bn3.weight 127 torch.Size([1024])\n",
      "129\n",
      "0.body.layer3.5.bn3.bias 128 torch.Size([1024])\n",
      "130\n",
      "0.body.layer4.0.conv1.weight 129 torch.Size([512, 1024, 1, 1])\n",
      "131\n",
      "0.body.layer4.0.bn1.weight 130 torch.Size([512])\n",
      "132\n",
      "0.body.layer4.0.bn1.bias 131 torch.Size([512])\n",
      "133\n",
      "0.body.layer4.0.conv2.weight 132 torch.Size([512, 512, 3, 3])\n",
      "134\n",
      "0.body.layer4.0.bn2.weight 133 torch.Size([512])\n",
      "135\n",
      "0.body.layer4.0.bn2.bias 134 torch.Size([512])\n",
      "136\n",
      "0.body.layer4.0.conv3.weight 135 torch.Size([2048, 512, 1, 1])\n",
      "137\n",
      "0.body.layer4.0.bn3.weight 136 torch.Size([2048])\n",
      "138\n",
      "0.body.layer4.0.bn3.bias 137 torch.Size([2048])\n",
      "139\n",
      "0.body.layer4.0.downsample.0.weight 138 torch.Size([2048, 1024, 1, 1])\n",
      "140\n",
      "0.body.layer4.0.downsample.1.weight 139 torch.Size([2048])\n",
      "141\n",
      "0.body.layer4.0.downsample.1.bias 140 torch.Size([2048])\n",
      "142\n",
      "0.body.layer4.1.conv1.weight 141 torch.Size([512, 2048, 1, 1])\n",
      "143\n",
      "0.body.layer4.1.bn1.weight 142 torch.Size([512])\n",
      "144\n",
      "0.body.layer4.1.bn1.bias 143 torch.Size([512])\n",
      "145\n",
      "0.body.layer4.1.conv2.weight 144 torch.Size([512, 512, 3, 3])\n",
      "146\n",
      "0.body.layer4.1.bn2.weight 145 torch.Size([512])\n",
      "147\n",
      "0.body.layer4.1.bn2.bias 146 torch.Size([512])\n",
      "148\n",
      "0.body.layer4.1.conv3.weight 147 torch.Size([2048, 512, 1, 1])\n",
      "149\n",
      "0.body.layer4.1.bn3.weight 148 torch.Size([2048])\n",
      "150\n",
      "0.body.layer4.1.bn3.bias 149 torch.Size([2048])\n",
      "151\n",
      "0.body.layer4.2.conv1.weight 150 torch.Size([512, 2048, 1, 1])\n",
      "152\n",
      "0.body.layer4.2.bn1.weight 151 torch.Size([512])\n",
      "153\n",
      "0.body.layer4.2.bn1.bias 152 torch.Size([512])\n",
      "154\n",
      "0.body.layer4.2.conv2.weight 153 torch.Size([512, 512, 3, 3])\n",
      "155\n",
      "0.body.layer4.2.bn2.weight 154 torch.Size([512])\n",
      "156\n",
      "0.body.layer4.2.bn2.bias 155 torch.Size([512])\n",
      "157\n",
      "0.body.layer4.2.conv3.weight 156 torch.Size([2048, 512, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "0.body.layer4.2.bn3.weight 157 torch.Size([2048])\n",
      "159\n",
      "0.body.layer4.2.bn3.bias 158 torch.Size([2048])\n",
      "160\n",
      "0.body.bn1.running_mean\n",
      "0.body.bn1.running_mean torch.Size([64]) 0\n",
      "0.body.bn1.running_var\n",
      "0.body.bn1.running_var torch.Size([64]) 1\n",
      "0.body.layer1.0.bn1.running_mean\n",
      "0.body.layer1.0.bn1.running_mean torch.Size([64]) 2\n",
      "0.body.layer1.0.bn1.running_var\n",
      "0.body.layer1.0.bn1.running_var torch.Size([64]) 3\n",
      "0.body.layer1.0.bn2.running_mean\n",
      "0.body.layer1.0.bn2.running_mean torch.Size([64]) 4\n",
      "0.body.layer1.0.bn2.running_var\n",
      "0.body.layer1.0.bn2.running_var torch.Size([64]) 5\n",
      "0.body.layer1.0.bn3.running_mean\n",
      "0.body.layer1.0.bn3.running_mean torch.Size([256]) 6\n",
      "0.body.layer1.0.bn3.running_var\n",
      "0.body.layer1.0.bn3.running_var torch.Size([256]) 7\n",
      "0.body.layer1.0.downsample.1.running_mean\n",
      "0.body.layer1.0.downsample.1.running_mean torch.Size([256]) 8\n",
      "0.body.layer1.0.downsample.1.running_var\n",
      "0.body.layer1.0.downsample.1.running_var torch.Size([256]) 9\n",
      "0.body.layer1.1.bn1.running_mean\n",
      "0.body.layer1.1.bn1.running_mean torch.Size([64]) 10\n",
      "0.body.layer1.1.bn1.running_var\n",
      "0.body.layer1.1.bn1.running_var torch.Size([64]) 11\n",
      "0.body.layer1.1.bn2.running_mean\n",
      "0.body.layer1.1.bn2.running_mean torch.Size([64]) 12\n",
      "0.body.layer1.1.bn2.running_var\n",
      "0.body.layer1.1.bn2.running_var torch.Size([64]) 13\n",
      "0.body.layer1.1.bn3.running_mean\n",
      "0.body.layer1.1.bn3.running_mean torch.Size([256]) 14\n",
      "0.body.layer1.1.bn3.running_var\n",
      "0.body.layer1.1.bn3.running_var torch.Size([256]) 15\n",
      "0.body.layer1.2.bn1.running_mean\n",
      "0.body.layer1.2.bn1.running_mean torch.Size([64]) 16\n",
      "0.body.layer1.2.bn1.running_var\n",
      "0.body.layer1.2.bn1.running_var torch.Size([64]) 17\n",
      "0.body.layer1.2.bn2.running_mean\n",
      "0.body.layer1.2.bn2.running_mean torch.Size([64]) 18\n",
      "0.body.layer1.2.bn2.running_var\n",
      "0.body.layer1.2.bn2.running_var torch.Size([64]) 19\n",
      "0.body.layer1.2.bn3.running_mean\n",
      "0.body.layer1.2.bn3.running_mean torch.Size([256]) 20\n",
      "0.body.layer1.2.bn3.running_var\n",
      "0.body.layer1.2.bn3.running_var torch.Size([256]) 21\n",
      "0.body.layer2.0.bn1.running_mean\n",
      "0.body.layer2.0.bn1.running_mean torch.Size([128]) 22\n",
      "0.body.layer2.0.bn1.running_var\n",
      "0.body.layer2.0.bn1.running_var torch.Size([128]) 23\n",
      "0.body.layer2.0.bn2.running_mean\n",
      "0.body.layer2.0.bn2.running_mean torch.Size([128]) 24\n",
      "0.body.layer2.0.bn2.running_var\n",
      "0.body.layer2.0.bn2.running_var torch.Size([128]) 25\n",
      "0.body.layer2.0.bn3.running_mean\n",
      "0.body.layer2.0.bn3.running_mean torch.Size([512]) 26\n",
      "0.body.layer2.0.bn3.running_var\n",
      "0.body.layer2.0.bn3.running_var torch.Size([512]) 27\n",
      "0.body.layer2.0.downsample.1.running_mean\n",
      "0.body.layer2.0.downsample.1.running_mean torch.Size([512]) 28\n",
      "0.body.layer2.0.downsample.1.running_var\n",
      "0.body.layer2.0.downsample.1.running_var torch.Size([512]) 29\n",
      "0.body.layer2.1.bn1.running_mean\n",
      "0.body.layer2.1.bn1.running_mean torch.Size([128]) 30\n",
      "0.body.layer2.1.bn1.running_var\n",
      "0.body.layer2.1.bn1.running_var torch.Size([128]) 31\n",
      "0.body.layer2.1.bn2.running_mean\n",
      "0.body.layer2.1.bn2.running_mean torch.Size([128]) 32\n",
      "0.body.layer2.1.bn2.running_var\n",
      "0.body.layer2.1.bn2.running_var torch.Size([128]) 33\n",
      "0.body.layer2.1.bn3.running_mean\n",
      "0.body.layer2.1.bn3.running_mean torch.Size([512]) 34\n",
      "0.body.layer2.1.bn3.running_var\n",
      "0.body.layer2.1.bn3.running_var torch.Size([512]) 35\n",
      "0.body.layer2.2.bn1.running_mean\n",
      "0.body.layer2.2.bn1.running_mean torch.Size([128]) 36\n",
      "0.body.layer2.2.bn1.running_var\n",
      "0.body.layer2.2.bn1.running_var torch.Size([128]) 37\n",
      "0.body.layer2.2.bn2.running_mean\n",
      "0.body.layer2.2.bn2.running_mean torch.Size([128]) 38\n",
      "0.body.layer2.2.bn2.running_var\n",
      "0.body.layer2.2.bn2.running_var torch.Size([128]) 39\n",
      "0.body.layer2.2.bn3.running_mean\n",
      "0.body.layer2.2.bn3.running_mean torch.Size([512]) 40\n",
      "0.body.layer2.2.bn3.running_var\n",
      "0.body.layer2.2.bn3.running_var torch.Size([512]) 41\n",
      "0.body.layer2.3.bn1.running_mean\n",
      "0.body.layer2.3.bn1.running_mean torch.Size([128]) 42\n",
      "0.body.layer2.3.bn1.running_var\n",
      "0.body.layer2.3.bn1.running_var torch.Size([128]) 43\n",
      "0.body.layer2.3.bn2.running_mean\n",
      "0.body.layer2.3.bn2.running_mean torch.Size([128]) 44\n",
      "0.body.layer2.3.bn2.running_var\n",
      "0.body.layer2.3.bn2.running_var torch.Size([128]) 45\n",
      "0.body.layer2.3.bn3.running_mean\n",
      "0.body.layer2.3.bn3.running_mean torch.Size([512]) 46\n",
      "0.body.layer2.3.bn3.running_var\n",
      "0.body.layer2.3.bn3.running_var torch.Size([512]) 47\n",
      "0.body.layer3.0.bn1.running_mean\n",
      "0.body.layer3.0.bn1.running_mean torch.Size([256]) 48\n",
      "0.body.layer3.0.bn1.running_var\n",
      "0.body.layer3.0.bn1.running_var torch.Size([256]) 49\n",
      "0.body.layer3.0.bn2.running_mean\n",
      "0.body.layer3.0.bn2.running_mean torch.Size([256]) 50\n",
      "0.body.layer3.0.bn2.running_var\n",
      "0.body.layer3.0.bn2.running_var torch.Size([256]) 51\n",
      "0.body.layer3.0.bn3.running_mean\n",
      "0.body.layer3.0.bn3.running_mean torch.Size([1024]) 52\n",
      "0.body.layer3.0.bn3.running_var\n",
      "0.body.layer3.0.bn3.running_var torch.Size([1024]) 53\n",
      "0.body.layer3.0.downsample.1.running_mean\n",
      "0.body.layer3.0.downsample.1.running_mean torch.Size([1024]) 54\n",
      "0.body.layer3.0.downsample.1.running_var\n",
      "0.body.layer3.0.downsample.1.running_var torch.Size([1024]) 55\n",
      "0.body.layer3.1.bn1.running_mean\n",
      "0.body.layer3.1.bn1.running_mean torch.Size([256]) 56\n",
      "0.body.layer3.1.bn1.running_var\n",
      "0.body.layer3.1.bn1.running_var torch.Size([256]) 57\n",
      "0.body.layer3.1.bn2.running_mean\n",
      "0.body.layer3.1.bn2.running_mean torch.Size([256]) 58\n",
      "0.body.layer3.1.bn2.running_var\n",
      "0.body.layer3.1.bn2.running_var torch.Size([256]) 59\n",
      "0.body.layer3.1.bn3.running_mean\n",
      "0.body.layer3.1.bn3.running_mean torch.Size([1024]) 60\n",
      "0.body.layer3.1.bn3.running_var\n",
      "0.body.layer3.1.bn3.running_var torch.Size([1024]) 61\n",
      "0.body.layer3.2.bn1.running_mean\n",
      "0.body.layer3.2.bn1.running_mean torch.Size([256]) 62\n",
      "0.body.layer3.2.bn1.running_var\n",
      "0.body.layer3.2.bn1.running_var torch.Size([256]) 63\n",
      "0.body.layer3.2.bn2.running_mean\n",
      "0.body.layer3.2.bn2.running_mean torch.Size([256]) 64\n",
      "0.body.layer3.2.bn2.running_var\n",
      "0.body.layer3.2.bn2.running_var torch.Size([256]) 65\n",
      "0.body.layer3.2.bn3.running_mean\n",
      "0.body.layer3.2.bn3.running_mean torch.Size([1024]) 66\n",
      "0.body.layer3.2.bn3.running_var\n",
      "0.body.layer3.2.bn3.running_var torch.Size([1024]) 67\n",
      "0.body.layer3.3.bn1.running_mean\n",
      "0.body.layer3.3.bn1.running_mean torch.Size([256]) 68\n",
      "0.body.layer3.3.bn1.running_var\n",
      "0.body.layer3.3.bn1.running_var torch.Size([256]) 69\n",
      "0.body.layer3.3.bn2.running_mean\n",
      "0.body.layer3.3.bn2.running_mean torch.Size([256]) 70\n",
      "0.body.layer3.3.bn2.running_var\n",
      "0.body.layer3.3.bn2.running_var torch.Size([256]) 71\n",
      "0.body.layer3.3.bn3.running_mean\n",
      "0.body.layer3.3.bn3.running_mean torch.Size([1024]) 72\n",
      "0.body.layer3.3.bn3.running_var\n",
      "0.body.layer3.3.bn3.running_var torch.Size([1024]) 73\n",
      "0.body.layer3.4.bn1.running_mean\n",
      "0.body.layer3.4.bn1.running_mean torch.Size([256]) 74\n",
      "0.body.layer3.4.bn1.running_var\n",
      "0.body.layer3.4.bn1.running_var torch.Size([256]) 75\n",
      "0.body.layer3.4.bn2.running_mean\n",
      "0.body.layer3.4.bn2.running_mean torch.Size([256]) 76\n",
      "0.body.layer3.4.bn2.running_var\n",
      "0.body.layer3.4.bn2.running_var torch.Size([256]) 77\n",
      "0.body.layer3.4.bn3.running_mean\n",
      "0.body.layer3.4.bn3.running_mean torch.Size([1024]) 78\n",
      "0.body.layer3.4.bn3.running_var\n",
      "0.body.layer3.4.bn3.running_var torch.Size([1024]) 79\n",
      "0.body.layer3.5.bn1.running_mean\n",
      "0.body.layer3.5.bn1.running_mean torch.Size([256]) 80\n",
      "0.body.layer3.5.bn1.running_var\n",
      "0.body.layer3.5.bn1.running_var torch.Size([256]) 81\n",
      "0.body.layer3.5.bn2.running_mean\n",
      "0.body.layer3.5.bn2.running_mean torch.Size([256]) 82\n",
      "0.body.layer3.5.bn2.running_var\n",
      "0.body.layer3.5.bn2.running_var torch.Size([256]) 83\n",
      "0.body.layer3.5.bn3.running_mean\n",
      "0.body.layer3.5.bn3.running_mean torch.Size([1024]) 84\n",
      "0.body.layer3.5.bn3.running_var\n",
      "0.body.layer3.5.bn3.running_var torch.Size([1024]) 85\n",
      "0.body.layer4.0.bn1.running_mean\n",
      "0.body.layer4.0.bn1.running_mean torch.Size([512]) 86\n",
      "0.body.layer4.0.bn1.running_var\n",
      "0.body.layer4.0.bn1.running_var torch.Size([512]) 87\n",
      "0.body.layer4.0.bn2.running_mean\n",
      "0.body.layer4.0.bn2.running_mean torch.Size([512]) 88\n",
      "0.body.layer4.0.bn2.running_var\n",
      "0.body.layer4.0.bn2.running_var torch.Size([512]) 89\n",
      "0.body.layer4.0.bn3.running_mean\n",
      "0.body.layer4.0.bn3.running_mean torch.Size([2048]) 90\n",
      "0.body.layer4.0.bn3.running_var\n",
      "0.body.layer4.0.bn3.running_var torch.Size([2048]) 91\n",
      "0.body.layer4.0.downsample.1.running_mean\n",
      "0.body.layer4.0.downsample.1.running_mean torch.Size([2048]) 92\n",
      "0.body.layer4.0.downsample.1.running_var\n",
      "0.body.layer4.0.downsample.1.running_var torch.Size([2048]) 93\n",
      "0.body.layer4.1.bn1.running_mean\n",
      "0.body.layer4.1.bn1.running_mean torch.Size([512]) 94\n",
      "0.body.layer4.1.bn1.running_var\n",
      "0.body.layer4.1.bn1.running_var torch.Size([512]) 95\n",
      "0.body.layer4.1.bn2.running_mean\n",
      "0.body.layer4.1.bn2.running_mean torch.Size([512]) 96\n",
      "0.body.layer4.1.bn2.running_var\n",
      "0.body.layer4.1.bn2.running_var torch.Size([512]) 97\n",
      "0.body.layer4.1.bn3.running_mean\n",
      "0.body.layer4.1.bn3.running_mean torch.Size([2048]) 98\n",
      "0.body.layer4.1.bn3.running_var\n",
      "0.body.layer4.1.bn3.running_var torch.Size([2048]) 99\n",
      "0.body.layer4.2.bn1.running_mean\n",
      "0.body.layer4.2.bn1.running_mean torch.Size([512]) 100\n",
      "0.body.layer4.2.bn1.running_var\n",
      "0.body.layer4.2.bn1.running_var torch.Size([512]) 101\n",
      "0.body.layer4.2.bn2.running_mean\n",
      "0.body.layer4.2.bn2.running_mean torch.Size([512]) 102\n",
      "0.body.layer4.2.bn2.running_var\n",
      "0.body.layer4.2.bn2.running_var torch.Size([512]) 103\n",
      "0.body.layer4.2.bn3.running_mean\n",
      "0.body.layer4.2.bn3.running_mean torch.Size([2048]) 104\n",
      "0.body.layer4.2.bn3.running_var\n",
      "0.body.layer4.2.bn3.running_var torch.Size([2048]) 105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import *\n",
    "class NestedTensor(object):\n",
    "\n",
    "    def __init__(self, tensor, mask):\n",
    "        self.mask = mask\n",
    "        self.tensors = tensor\n",
    "\n",
    "from models.backbone import *\n",
    "filepath = '/private/home/padentomasello/scratch/pytorch_testing/detr_backbone.array'\n",
    "\n",
    "N = 2\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "embedding_size = 8\n",
    "tgt_len = 10\n",
    "\n",
    "queries = torch.rand(tgt_len, embedding_size)\n",
    "image = torch.rand(N, C, H, W)\n",
    "mask = torch.zeros(N, H, W)\n",
    "#mask[0, :20, :20] = 1\n",
    "##mask[1, :4, :10] = 1\n",
    "\n",
    "\n",
    "\n",
    "af.array.save_array('image', toArrayFire(image), filepath, False)\n",
    "#af.array.save_array('queries', toArrayFire(queries), filepath, True)\n",
    "af.array.save_array('mask', toArrayFire(mask), filepath, True)\n",
    "#af.array.save_array('pos', toArrayFire(pos), filepath, True)\n",
    "       \n",
    "\n",
    "\n",
    "model.eval()\n",
    "output = model(NestedTensor(image, mask.to(bool)))[0][0].tensors\n",
    "saveStateDict(model, filepath)\n",
    "af.array.save_array('output', toArrayFire(output), filepath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8564)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2048, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'class_embed.weight', 'class_embed.bias', 'bbox_embed.layers.0.weight', 'bbox_embed.layers.0.bias', 'bbox_embed.layers.1.weight', 'bbox_embed.layers.1.bias', 'bbox_embed.layers.2.weight', 'bbox_embed.layers.2.bias', 'query_embed.weight', 'input_proj.weight', 'input_proj.bias', 'backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'backbone.0.body.layer4.0.conv1.weight', 'backbone.0.body.layer4.0.bn1.weight', 'backbone.0.body.layer4.0.bn1.bias', 'backbone.0.body.layer4.0.bn1.running_mean', 'backbone.0.body.layer4.0.bn1.running_var', 'backbone.0.body.layer4.0.conv2.weight', 'backbone.0.body.layer4.0.bn2.weight', 'backbone.0.body.layer4.0.bn2.bias', 'backbone.0.body.layer4.0.bn2.running_mean', 'backbone.0.body.layer4.0.bn2.running_var', 'backbone.0.body.layer4.0.conv3.weight', 'backbone.0.body.layer4.0.bn3.weight', 'backbone.0.body.layer4.0.bn3.bias', 'backbone.0.body.layer4.0.bn3.running_mean', 'backbone.0.body.layer4.0.bn3.running_var', 'backbone.0.body.layer4.0.downsample.0.weight', 'backbone.0.body.layer4.0.downsample.1.weight', 'backbone.0.body.layer4.0.downsample.1.bias', 'backbone.0.body.layer4.0.downsample.1.running_mean', 'backbone.0.body.layer4.0.downsample.1.running_var', 'backbone.0.body.layer4.1.conv1.weight', 'backbone.0.body.layer4.1.bn1.weight', 'backbone.0.body.layer4.1.bn1.bias', 'backbone.0.body.layer4.1.bn1.running_mean', 'backbone.0.body.layer4.1.bn1.running_var', 'backbone.0.body.layer4.1.conv2.weight', 'backbone.0.body.layer4.1.bn2.weight', 'backbone.0.body.layer4.1.bn2.bias', 'backbone.0.body.layer4.1.bn2.running_mean', 'backbone.0.body.layer4.1.bn2.running_var', 'backbone.0.body.layer4.1.conv3.weight', 'backbone.0.body.layer4.1.bn3.weight', 'backbone.0.body.layer4.1.bn3.bias', 'backbone.0.body.layer4.1.bn3.running_mean', 'backbone.0.body.layer4.1.bn3.running_var', 'backbone.0.body.layer4.2.conv1.weight', 'backbone.0.body.layer4.2.bn1.weight', 'backbone.0.body.layer4.2.bn1.bias', 'backbone.0.body.layer4.2.bn1.running_mean', 'backbone.0.body.layer4.2.bn1.running_var', 'backbone.0.body.layer4.2.conv2.weight', 'backbone.0.body.layer4.2.bn2.weight', 'backbone.0.body.layer4.2.bn2.bias', 'backbone.0.body.layer4.2.bn2.running_mean', 'backbone.0.body.layer4.2.bn2.running_var', 'backbone.0.body.layer4.2.conv3.weight', 'backbone.0.body.layer4.2.bn3.weight', 'backbone.0.body.layer4.2.bn3.bias', 'backbone.0.body.layer4.2.bn3.running_mean', 'backbone.0.body.layer4.2.bn3.running_var'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d34bded2808c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "output = model(NestedTensor(x, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0].tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
